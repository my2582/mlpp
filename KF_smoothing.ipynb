{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:07.982375Z",
     "start_time": "2019-11-15T00:18:07.147515Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading a pre-processed data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:08.026589Z",
     "start_time": "2019-11-15T00:18:07.985161Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './dataset/toyset/'\n",
    "filename = 'processed_toyset.csv'\n",
    "df = pd.read_csv(path + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset row/column sizes to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:08.031756Z",
     "start_time": "2019-11-15T00:18:08.028906Z"
    }
   },
   "outputs": [],
   "source": [
    "nrow = 3\n",
    "ncol = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:08.038279Z",
     "start_time": "2019-11-15T00:18:08.034795Z"
    }
   },
   "outputs": [],
   "source": [
    "proc_covar = 0.0001**2*np.identity(nrow)\n",
    "post_covar = proc_covar\n",
    "meas_covar = 0.0003**2*np.identity(nrow)\n",
    "pred_covar = meas_covar\n",
    "ident = np.identity(nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:08.047475Z",
     "start_time": "2019-11-15T00:18:08.040471Z"
    }
   },
   "outputs": [],
   "source": [
    "state_old = np.ones((3, 1))\n",
    "emit_mat = np.array([[1., -1, 0.],\n",
    "                      [1.,  0, -1.],\n",
    "                      [0., -1., 1.],\n",
    "                      ])\n",
    "obs_mat = df.drop(['timestamp', 'time_gap'], axis=1)\n",
    "obs_mat = obs_mat.to_numpy()\n",
    "time_gap = df.time_gap.to_numpy()\n",
    "\n",
    "latent_states = np.empty(shape=(df.shape[0], nrow))\n",
    "predicted_obs = np.empty(shape=(df.shape[0], nrow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumptions:\n",
    "- the order of measurements is GBPJPY, GBPUSD, USDJPY.\n",
    "- the order of latent variables is GBP, JPY, USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:08.052334Z",
     "start_time": "2019-11-15T00:18:08.049522Z"
    }
   },
   "outputs": [],
   "source": [
    "ccy_list = ['gbpjpy', 'gbpusd', 'usdjpy']\n",
    "latent_cols = ['GBP', 'JPY', 'USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:09.695052Z",
     "start_time": "2019-11-15T00:18:08.054774Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    post_covar = post_covar + time_gap[i]*proc_covar\n",
    "\n",
    "    # obs_mat[i,:] is a (3,) vector. So, we need flatten() to match\n",
    "    innovation = obs_mat[i, :] - (emit_mat@state_old).flatten()\n",
    "    innovation_covar = emit_mat@post_covar@(emit_mat.T) + meas_covar\n",
    "    kalman_gain = post_covar@(emit_mat.T)@np.linalg.inv(innovation_covar)\n",
    "\n",
    "    # We transpose (kalman_gain@innovation) from (3,) to (3,1)\n",
    "    state_new = state_old + (kalman_gain@innovation).reshape(-1, 1)\n",
    "    post_covar = (ident - kalman_gain@emit_mat)@post_covar\n",
    "    predicted_obs[i] = (emit_mat@state_new).T\n",
    "    latent_states[i] - state_new.T\n",
    "    state_old = state_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the oldest timestamp and shift each row upward by one row.\n",
    "- e.g. If the raw dataset starts from 2019-05-01 00:00:00 at 0th element, predictions would start from 2019-05-01 00:00:01 at 0th element instead ('lead' by one minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:09.709689Z",
     "start_time": "2019-11-15T00:18:09.698861Z"
    }
   },
   "outputs": [],
   "source": [
    "new_idx = df.timestamp.shift(-1).to_numpy().reshape(-1, 1)\n",
    "predictions = np.hstack((new_idx, predicted_obs))\n",
    "latent_estimates = np.hstack((new_idx, latent_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:09.717368Z",
     "start_time": "2019-11-15T00:18:09.712449Z"
    }
   },
   "outputs": [],
   "source": [
    "assert predictions.shape[0] == predicted_obs.shape[0], \"We have some data loss.\"\n",
    "assert latent_estimates.shape[0] == latent_states.shape[0], \"We have some data loss.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert arrays to DataFrame instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:09.738055Z",
     "start_time": "2019-11-15T00:18:09.719644Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(predictions, columns=['timestamp'] + ccy_list)\n",
    "latent_estimates = pd.DataFrame(latent_estimates,\n",
    "                                columns=['timestampe'] + latent_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good are the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's worse than the baseline. Let's see what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, currency exchange rates are extremely volatile, and our naive approach fails. Then again, the covariance matrices are not estimated in this case, and there may be a setting that works better. But on the bright side, the graph of currency intrinsic values is really neat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation: EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:18:38.509033Z",
     "start_time": "2019-11-15T00:18:38.499463Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(data, initial_state, emit_mat, meas_covar, proc_covar):    \n",
    "    '''\n",
    "    Parameters:\n",
    "      data(DataFrame, ): a (batch) dataset\n",
    "    '''\n",
    "    obs_mat = data.drop(['timestamp', 'time_gap'], axis=1)\n",
    "    obs_mat = obs_mat.to_numpy()\n",
    "    time_gap = data.time_gap.to_numpy().reshape(-1,1)\n",
    "\n",
    "    latent_states = np.empty(shape=(data.shape[0], nrow))\n",
    "    predicted_obs = np.empty(shape=(data.shape[0], nrow))\n",
    "\n",
    "    state_old = initial_state    \n",
    "    post_covar = proc_covar\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        post_covar = post_covar + time_gap[i]*proc_covar\n",
    "\n",
    "        # obs_mat[i,:] is a (3,) vector. So, we need flatten() to match\n",
    "        innovation = obs_mat[i, :] - (emit_mat@state_old).flatten()\n",
    "        innovation_covar = emit_mat@post_covar@(emit_mat.T) + meas_covar\n",
    "        kalman_gain = post_covar@(emit_mat.T)@np.linalg.inv(innovation_covar)\n",
    "\n",
    "        # We transpose (kalman_gain@innovation) from (3,) to (3,1)\n",
    "        state_new = state_old + (kalman_gain@innovation).reshape(-1, 1)\n",
    "        post_covar = (ident - kalman_gain@emit_mat)@post_covar\n",
    "        predicted_obs[i] = (emit_mat@state_new).T\n",
    "        latent_states[i] - state_new.T\n",
    "        state_old = state_new\n",
    "        \n",
    "#     # Convert np arrays to DataFrame instances\n",
    "#     new_idx = data.timestamp.shift(-1)\n",
    "#     na_idx = new_idx.index[new_idx.isna() == True]\n",
    "\n",
    "#     # We exclude any row with nan when we create a timestamp index\n",
    "#     new_idx = new_idx[na_idx]\n",
    "#     predicted_obs = predicted_obs[na_idx]\n",
    "#     time_gap = time_gap[na_idx]\n",
    "    new_idx = data.timestamp.shift(-1).to_numpy().reshape(-1, 1)\n",
    "    predictions = np.hstack((new_idx, predicted_obs))\n",
    "    predictions = np.hstack((predictions, time_gap))\n",
    "    latent_estimates = np.hstack((new_idx, latent_states))\n",
    "    latent_estimates = np.hstack((latent_estimates, time_gap))\n",
    "    \n",
    "    return predictions, latent_estimates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:46:20.614639Z",
     "start_time": "2019-11-15T00:46:20.599630Z"
    }
   },
   "outputs": [],
   "source": [
    "training = df.loc[10001:11000, :]\n",
    "\n",
    "predictions, latent_estimates = forward_pass(data=training[:100],\n",
    "                                             initial_state=np.ones((3, 1)),\n",
    "                                             emit_mat=emit_mat,\n",
    "                                             meas_covar=meas_covar,\n",
    "                                             proc_covar=proc_covar)\n",
    "\n",
    "# Convert to pandas Data Frame\n",
    "predictions = pd.DataFrame(predictions,\n",
    "                           columns=['timestamp'] + ccy_list + ['time_gap'])\n",
    "\n",
    "latent_estimates = pd.DataFrame(latent_estimates,\n",
    "                                columns=['timestamp'] + latent_cols + ['time_gap'])\n",
    "\n",
    "check = []\n",
    "check.append(predictions)\n",
    "check.append(latent_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T00:46:21.833352Z",
     "start_time": "2019-11-15T00:46:21.818690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>gbpjpy</th>\n",
       "      <th>gbpusd</th>\n",
       "      <th>usdjpy</th>\n",
       "      <th>time_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-09 17:42:00</td>\n",
       "      <td>1.98516</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>1.87969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-09 17:43:00</td>\n",
       "      <td>3.24472</td>\n",
       "      <td>0.172356</td>\n",
       "      <td>3.07237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-09 17:44:00</td>\n",
       "      <td>3.98418</td>\n",
       "      <td>0.211683</td>\n",
       "      <td>3.7725</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-09 17:45:00</td>\n",
       "      <td>4.40759</td>\n",
       "      <td>0.23423</td>\n",
       "      <td>4.17336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-09 17:46:00</td>\n",
       "      <td>4.64805</td>\n",
       "      <td>0.247042</td>\n",
       "      <td>4.40101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2019-05-09 19:16:00</td>\n",
       "      <td>4.96242</td>\n",
       "      <td>0.264643</td>\n",
       "      <td>4.69777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2019-05-09 19:17:00</td>\n",
       "      <td>4.96239</td>\n",
       "      <td>0.264608</td>\n",
       "      <td>4.69778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2019-05-09 19:18:00</td>\n",
       "      <td>4.96236</td>\n",
       "      <td>0.264573</td>\n",
       "      <td>4.69778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2019-05-09 19:19:00</td>\n",
       "      <td>4.96239</td>\n",
       "      <td>0.264568</td>\n",
       "      <td>4.69782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2019-05-09 19:20:00</td>\n",
       "      <td>4.9624</td>\n",
       "      <td>0.264609</td>\n",
       "      <td>4.69779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp   gbpjpy    gbpusd   usdjpy time_gap\n",
       "0   2019-05-09 17:42:00  1.98516  0.105469  1.87969        1\n",
       "1   2019-05-09 17:43:00  3.24472  0.172356  3.07237        1\n",
       "2   2019-05-09 17:44:00  3.98418  0.211683   3.7725        1\n",
       "3   2019-05-09 17:45:00  4.40759   0.23423  4.17336        1\n",
       "4   2019-05-09 17:46:00  4.64805  0.247042  4.40101        1\n",
       "..                  ...      ...       ...      ...      ...\n",
       "94  2019-05-09 19:16:00  4.96242  0.264643  4.69777        1\n",
       "95  2019-05-09 19:17:00  4.96239  0.264608  4.69778        1\n",
       "96  2019-05-09 19:18:00  4.96236  0.264573  4.69778        1\n",
       "97  2019-05-09 19:19:00  4.96239  0.264568  4.69782        1\n",
       "98  2019-05-09 19:20:00   4.9624  0.264609  4.69779        1\n",
       "\n",
       "[99 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T05:15:17.304380Z",
     "start_time": "2019-11-15T05:15:17.294005Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_proc_covar(latent_estimates):\n",
    "    # drop any row with na\n",
    "    latent_estimates = latent_estimates.dropna(how='any')\n",
    "    time_gap = latent_estimates.time_gap\n",
    "    \n",
    "    # drop two columns\n",
    "    latent_estimates = latent_estimates.drop(['timestamp', 'time_gap'], axis=1)\n",
    "\n",
    "    sample_means = [np.average(latent_estimates[ccy], weights=time_gap) for ccy in latent_cols]\n",
    "    latent_estimates = latent_estimates[latent_cols] - sample_means\n",
    "    \n",
    "    return (latent_estimates.T)@latent_estimates\n",
    "\n",
    "def update_meas_covar(pred, obs):\n",
    "    unique_timestamp = np.sort(np.unique(np.union1d(pred['timestamp'].dropna(\n",
    "        how='any').to_numpy(), obs['timestamp'].dropna(how='any').to_numpy())))\n",
    "\n",
    "    obs_mat = obs[np.isin(obs.timestamp, unique_timestamp)]\n",
    "    pred_mat = pred[np.isin(pred.timestamp, unique_timestamp)]\n",
    "\n",
    "    obs_pred_mat = pd.merge(left=obs_mat, right=pred_mat,\n",
    "                            how='inner', on='timestamp', suffixes=('_obs', '_pred'))\n",
    "\n",
    "    ccy_list_obs = [ccy+'_obs' for ccy in ccy_list]\n",
    "    ccy_list_pred = [ccy+'_pred' for ccy in ccy_list]\n",
    "\n",
    "    error_mat = np.array(obs_pred_mat[ccy_list_pred], dtype=float) - np.array(obs_pred_mat[ccy_list_obs], dtype=float) \n",
    "    cov_mat = (error_mat.T)@error_mat\n",
    "    \n",
    "    return cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T05:16:02.440696Z",
     "start_time": "2019-11-15T05:16:02.415532Z"
    }
   },
   "outputs": [],
   "source": [
    "check.append(update_proc_covar(check[1]))\n",
    "check.append(update_meas_covar(pred=check[0], obs=training[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T05:29:55.276478Z",
     "start_time": "2019-11-15T05:29:55.269132Z"
    }
   },
   "outputs": [],
   "source": [
    "def kalman_smoother(data, prior_state, meas_mat, prior_df, prior_meas_scale, prior_proc_scale):\n",
    "    predictions, latent_estimates = forward_pass(data=data, initial_state=prior_state,\n",
    "                           emit_mat=emit_mat, meas_cover=prior_meas_scale/prior_df,\n",
    "                           proc_covar=prior_proce_scale/proir_df)\n",
    "    \n",
    "    proc_covar_new = update_proc_covar(latent_estimates)\n",
    "    meas_covar_new = update_meas_covar(pred=predictions, obs=data)\n",
    "    \n",
    "    proc_covar_old = proc_covar_new*2\n",
    "    meas_covar_old = meas_covar_new*2\n",
    "    \n",
    "    while np.sqrt(np.sum((proc_covar_old-proc_covar_new)^2) +\n",
    "                  np.sum((meas_covar_old-meas_covar_new)^2)) > 10^(-16):\n",
    "        print(np.sum((proc_covar_old - proc_covar_new)^2) + np.sum((meas_covar_old - meas_covar_new)^2))\n",
    "        \n",
    "        meas_covar_old = meas_covar_new\n",
    "        proc_covar_old = proc_covar_new\n",
    "        \n",
    "        predictions, latent_estimates = forward_pass(data=data, initial_state=prior_state,\n",
    "                           emit_mat=emit_mat, meas_cover=meas_covar_new,\n",
    "                           proc_covar=proc_covar_new)\n",
    "        \n",
    "        proc_covar_new = update_proc_covar(latent_estimates)\n",
    "        meas_covar_new = update_meas_covar(pred=predictions, obs=data)\n",
    "    \n",
    "    new_df = prior_df + data.shape[0] - emit_mat.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
