---
title: "KF prototype"
output: pdf_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=4) 
```

# Cleaning the data

```{r, warning = FALSE, message = FALSE}
library(data.table)
library(lubridate)
library(zoo)
library(ggplot2)

var_names <- c('timestamp',  'opening', 'high', 'low', 'close', 'volume')
dir <- './dataset/toyset/'

custom_merge <- function(x, y){
  return(merge(x, y, by = 'timestamp', all = TRUE))
}

create_timestamps <- function(start_date, end_date, opening_time, closing_time){
  date_seq <- seq.Date(start_date, end_date, by = 'day')
  temp <- list()
  for(i in 1:length(date_seq)){
    if(!is.na(opening_time[wday(date_seq[i])])){
      temp[[i]] <- data.table(
        timestamp = seq.POSIXt(
          ymd_hms(paste(date_seq[i], opening_time[wday(date_seq[i])])), 
          ymd_hms(paste(date_seq[i], closing_time[wday(date_seq[i])])),
          by = 'min'
        )
      )
    }
  }
  return(rbindlist(temp))
}

get_data <- function(dir){
  filenames <- list.files(dir)
  filenames <- filenames[!grepl("processed", filenames)]
  dat <- list()
  for(i in 1:length(filenames)){
    temp_dat <- fread(paste0(dir, filenames[i]))
    setnames(temp_dat, var_names)
    temp_dat2 <- temp_dat[,.(
      timestamp = as.POSIXct(timestamp, format = "%Y%m%d %H%M%OS", tz = 'EST'),
      price = log(close)
      )]
    setnames(temp_dat2, c('timestamp', substr(filenames[i], 1, 6)))
    dat[[i]] <- temp_dat2
  }
  final_dat <- Reduce(custom_merge, dat)
  
  daily_times <- final_dat[
    ,.(opening_time = substr(timestamp, 12, 19), 
       closing_time = substr(timestamp, 12, 19),
       timestamp = as.Date(timestamp, tz = 'EST'))]
  daily_times <- daily_times[
    ,.(opening_time = head(sort(opening_time), 1),
       closing_time = tail(sort(closing_time), 1)),
    by = .(dow = wday(timestamp))
  ]
  daily_times <- daily_times[order(dow)]
  
  timestamp_list <- create_timestamps(
    start_date   = min(as.Date(final_dat$timestamp, tz = 'EST')),
    end_date     = max(as.Date(final_dat$timestamp, tz = 'EST')),
    opening_time = daily_times$opening_time[match(1:7, daily_times$dow)],
    closing_time = daily_times$closing_time[match(1:7, daily_times$dow)]
  )
  
  final_dat <- merge(timestamp_list, final_dat, by = 'timestamp', all = TRUE)
  final_dat <- final_dat[,lapply(.SD, na.locf, na.rm = FALSE, fromLast = TRUE)]
  return(final_dat)
}

dat <- get_data(dir)
dat[,time_gap := (as.numeric(timestamp) - as.numeric(shift(timestamp)))/60]
dat$time_gap[1] <- 1
head(dat)
```

```{r}
write.csv(dat, './dataset/toyset/processed_toyset.csv', row.names = FALSE)
```

# Kalman Filter with known covariances

Here we assume independence in movements, just to get a simple prototype to work. proc_covar and meas_covar are the main settings. When proc_covar is relatively small when compared to meas_covar, then the filter will be smooth.

```{r}
proc_covar <- matrix(0, nrow = 3, ncol = 3)
diag(proc_covar) <- 0.0001^2

post_covar <- proc_covar

meas_covar <- matrix(0, nrow = 3, ncol = 3)
diag(meas_covar) <- 0.0003^2

pred_covar <- meas_covar

ident <- matrix(0, nrow = 3, ncol = 3)
diag(ident) <- 1

# assume the order of measurements is GBPJPY, GBPUSD, USDJPY
# and order of latent variables is GBP, JPY, USD

state_old <- matrix(rep(1, 3), ncol = 1)
trans_mat <- matrix(c(
  1, -1, 0,
  1, 0, -1,
  0, -1, 1
), nrow = 3, ncol = 3, byrow = TRUE)

obs_mat <- as.matrix(dat[,!colnames(dat) %in% c('timestamp', 'time_gap'), with = FALSE])
time_gap <- dat$time_gap

latent_states <- list()
predicted_obs <- list()

for(i in 1:nrow(dat)){
  post_covar <- post_covar + time_gap[i] * proc_covar
  innovation <- obs_mat[i,] - trans_mat %*% state_old
  tryCatch()
  innovation_covar <- trans_mat %*% post_covar %*% t(trans_mat) + meas_covar
  kalman_gain <- post_covar %*% t(trans_mat) %*% solve(innovation_covar)
  state_new <- state_old + kalman_gain %*% innovation
  post_covar <- (ident - kalman_gain %*% trans_mat) %*% post_covar
  predicted_obs[[i]] <- t(trans_mat %*% state_new)
  latent_states[[i]] <- t(state_new)
  state_old <- state_new
}

predictions <- data.table(do.call(rbind, predicted_obs))
setnames(predictions, colnames(obs_mat))
predictions$timestamp <- shift(dat$timestamp, n = 1, type = 'lead')

latent_estimates <- data.table(do.call(rbind, latent_states))
setnames(latent_estimates, c('GBP', 'JPY', 'USD'))
latent_estimates$timestamp <- shift(dat$timestamp, n = 1, type = 'lead')
```

## How good are the predictions?

```{r}
# baseline of one-step ahead forecast
baseline <- data.table(
  gbpjpy = dat$gbpjpy - shift(dat$gbpjpy, n = 1, type = 'lead'),
  gbpusd = dat$gbpusd - shift(dat$gbpusd, n = 1, type = 'lead'),
  usdjpy = dat$usdjpy - shift(dat$usdjpy, n = 1, type = 'lead')
)
baseline <- baseline[200:nrow(baseline)]
baseline[,lapply(.SD, function(x) sqrt(mean(x^2, na.rm = TRUE)))]

predicted <- data.table(
  gbpjpy = predictions$gbpjpy - shift(dat$gbpjpy, n = 1, type = 'lead'),
  gbpusd = predictions$gbpusd - shift(dat$gbpusd, n = 1, type = 'lead'),
  usdjpy = predictions$usdjpy - shift(dat$usdjpy, n = 1, type = 'lead')
)
predicted <- predicted[200:nrow(predicted)]
predicted[,lapply(.SD, function(x) sqrt(mean(x^2, na.rm = TRUE)))]


```

It's worse than the baseline. Let's see what's going on:

```{r}
viz_dat <- merge(
  melt(dat[,.(timestamp, gbpjpy, gbpusd, usdjpy)], 
       id.vars = 'timestamp', 
       variable.name = 'currency',
       value.name = 'actual'), 
  melt(predictions[,.(timestamp, gbpjpy, gbpusd, usdjpy)],
       id.vars = 'timestamp',
       variable.name = 'currency',
       value.name = 'predicted'),
  by = c('timestamp', 'currency')
)
ggplot(viz_dat[currency == 'gbpusd' &
                 timestamp >= '2019-05-14 12:00:00' & 
                 timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = actual), color = 'blue') +
  geom_line(aes(x = timestamp, y = predicted), color = 'red') +
  ggtitle('GBPUSD')


ggplot(viz_dat[currency == 'gbpjpy' &
                 timestamp >= '2019-05-14 12:00:00' & 
                 timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = actual), color = 'blue') +
  geom_line(aes(x = timestamp, y = predicted), color = 'red') +
  ggtitle('GBPJPY')


ggplot(viz_dat[currency == 'usdjpy' &
                 timestamp >= '2019-05-14 12:00:00' & 
                 timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = actual), color = 'blue') +
  geom_line(aes(x = timestamp, y = predicted), color = 'red') +
  ggtitle('USDJPY')
```

As we expected, currency exchange rates are extremely volatile, and our naive approach fails. Then again, the covariance matrices are not estimated in this case, and there may be a setting that works better. But on the bright side, the graph of currency intrinsic values is really neat:

```{r}
latent_viz <- melt(
  latent_estimates,
  id.vars = 'timestamp',
  variable.name = 'currency',
  value.name = 'log_intrinsic_value'
)
ggplot(latent_viz[currency == 'JPY' &
                  timestamp >= '2019-05-14 12:00:00' & 
                    timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = log_intrinsic_value)) +
  ggtitle('Intrinsic value of JPY')

ggplot(latent_viz[currency == 'USD' &
                  timestamp >= '2019-05-14 12:00:00' & 
                    timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = log_intrinsic_value)) +
  ggtitle('Intrinsic value of USD')

ggplot(latent_viz[currency == 'GBP' &
                  timestamp >= '2019-05-14 12:00:00' & 
                    timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = log_intrinsic_value)) +
  ggtitle('Intrinsic value of GBP')
```


# Parameter Estimation

So the goal of the project is to build an adaptive Kalman Filter. However, the problem is impossible at our current level, so we have to do a middle ground. A Kalman Smoother allows us to estimate the covariance matrices through the EM algorithm, but it requires using future data to predict the past. By combining a Kalman Smoother with PSIS detailed in the next section, we construct an algorithm to do filtering in a heuristic fashion.

```{r}
ggplot() +
  geom_line(data = training[20:100],
            aes(x = timestamp, y = gbpusd),
            color = 'blue') +
  geom_line(data = check_forward[[1]][20:100],
            aes(x = timestamp, y = gbpusd),
            color = 'red')
```

```{r}
rm(list = ls(all.names = TRUE))

dat <- fread('./dataset/toyset/processed_toyset.csv')
dat$timestamp <- as.POSIXct(dat$timestamp)
dat$gbpjpy <- dat$gbpjpy * 100
dat$gbpusd <- dat$gbpusd * 100
dat$usdjpy <- dat$usdjpy * 100
training <- dat[10001:12000,]
```

```{r}
trans_mat <- matrix(c(
  1, -1, 0,
  1, 0, -1,
  0, -1, 1
), nrow = 3, ncol = 3, byrow = TRUE)


# E step
# The forward pass propagates through the system, given fixed covariance matrices
# It outputs both the predicted states and the predicted measurements

forward_pass <- function(
  data, initial_state, trans_mat, meas_covar, proc_covar, post_covar
){
  obs_mat <- as.matrix(data[,!colnames(dat) %in% c('timestamp', 'time_gap'), 
                            with = FALSE])
  time_gap <- data$time_gap
  state_old <- initial_state
  ident <- diag(1, nrow = ncol(trans_mat))
  
  latent_states <- list()
  predicted_obs <- list()
  
  for(i in 1:nrow(data)){
    post_covar <- post_covar + time_gap[i] * proc_covar
    innovation <- obs_mat[i,] - trans_mat %*% state_old
    try_innov <- try(solve(trans_mat %*% post_covar %*% t(trans_mat) + meas_covar))
    if(is.numeric(try_innov)){
      innovation_covar <- trans_mat %*% post_covar %*% t(trans_mat) + meas_covar
    }
    kalman_gain <- post_covar %*% t(trans_mat) %*% solve(innovation_covar)
    state_new <- state_old + kalman_gain %*% innovation
    post_covar <- (ident - kalman_gain %*% trans_mat) %*% post_covar
    predicted_obs[[i]] <- t(trans_mat %*% state_new)
    latent_states[[i]] <- t(state_new)
    state_old <- state_new
  }
    
  predictions <- data.table(do.call(rbind, predicted_obs))
  setnames(predictions, colnames(obs_mat))
  predictions$timestamp <- shift(data$timestamp, n = 1, type = 'lead')
  predictions$time_gap <- time_gap
  
  latent_estimates <- data.table(do.call(rbind, latent_states))
  setnames(latent_estimates, c('GBP', 'JPY', 'USD')) # need to change this later
  latent_estimates$timestamp <- shift(data$timestamp, n = 1, type = 'lead')
  latent_estimates$time_gap <- time_gap
  
  return(list(predictions = predictions, 
              latent_estimates = latent_estimates, 
              post_covar = post_covar, 
              innovation_covar = innovation_covar))
}

# M step

sample_proc_scale <- function(data){
# This function simply takes the scale matrix of the propagated states
# But we have to take into account the uneven time gaps
  
  data <- data[,lapply(.SD, function(x) (x - shift(x, type = 'lag')) / time_gap)]
  data <- data[complete.cases(data)]
  data <- data[,timestamp := NULL]
  data <- data[,time_gap := NULL]
  proc_scale <- as.matrix(data)
  proc_scale <- t(proc_scale) %*% proc_scale
#  sample_means <- as.numeric(
#    data[,lapply(.SD, function(x) sum(x * shift(time_gap, type = 'lead'), na.rm = TRUE) / 
#        sum(shift(time_gap, type = 'lead'), na.rm = TRUE))]
#    )
#  sample_means <- as.numeric(data[,lapply(.SD, mean, na.rm = TRUE)])
#  proc_scale <- t((t(as.matrix(data)) - sample_means))
#  proc_scale <- t(proc_scale) %*% proc_scale
#  proc_scale <- t(proc_scale) %*% diag(1/time_gap) %*% proc_scale
  return(proc_scale)
}

sample_meas_scale <- function(pred, obs){
# This function gets us the observed scale matrix through (biased) MLE
# The debiasing is done by supplying the correct df to the invWishart

  unique_timestamp <- sort(unique(pred$timestamp, obs$timestamp))
  obs_mat <- obs[timestamp %in% unique_timestamp]
  pred_mat <- pred[timestamp %in% unique_timestamp]
  
  obs_mat <- as.matrix(obs_mat[order(timestamp), !c('timestamp', 'time_gap')])
  pred_mat <- as.matrix(pred_mat[order(timestamp),][, colnames(obs_mat), with = FALSE])
  
  error_mat <- obs_mat - pred_mat
  
  return(t(error_mat) %*% error_mat)
}

# EM
# This function runs the smoother on a time block through EM

kalman_smoother <- function(
  data, prior_state, meas_mat, post_covar,
  prior_meas_df, prior_meas_scale, 
  prior_proc_df, prior_proc_scale
){
  forward <- forward_pass(
    data = data, 
    initial_state = prior_state,
    trans_mat = meas_mat,
    meas_covar = prior_meas_scale / (prior_meas_df),
    proc_covar = prior_proc_scale / (prior_proc_df),
    post_covar = post_covar
  )
  post_proc_scale <- sample_proc_scale(forward[[2]])
  post_meas_scale <- sample_meas_scale(pred = forward[[1]], obs = data)
  
  old_proc_scale <- post_proc_scale * 2
  old_meas_scale <- post_meas_scale * 2
  
  while(sqrt(sum(((old_proc_scale - post_proc_scale) / nrow(data))^2) +
        sum(((old_meas_scale - post_meas_scale) / nrow(data))^2)) > 10^(-9)){
    
    old_meas_scale <- post_meas_scale
    old_proc_scale <- post_proc_scale
    
    forward <- forward_pass(
      data = data,
      initial_state = prior_state,
      trans_mat = trans_mat,
      meas_covar = post_meas_scale / nrow(data),
      proc_covar = post_proc_scale / nrow(data),
      post_covar = post_covar
    )
    
    post_proc_scale <- sample_proc_scale(forward[[2]])
    post_meas_scale <- sample_meas_scale(pred = forward[[1]], obs = data)
  }
  
  post_meas_df <- prior_meas_df + nrow(data) - nrow(trans_mat)
  post_proc_df <- prior_proc_df + nrow(data) - ncol(trans_mat)
  
  return(list(
    input_data = data,
    states = forward[[2]],
    predicted_measurements = forward[[1]],
    post_meas_df = post_meas_df, 
    post_meas_scale = prior_meas_scale + post_meas_scale,
    post_proc_df = post_proc_df, 
    post_proc_scale = prior_proc_scale + post_proc_scale,
    post_covar = forward[[3]],
    innovation_covar = forward[[4]]
  ))
}
```


```{r}
check_forward <- forward_pass(
  data = training[1:100],
  initial_state = c(2.7, -2.2, 2.5),
  trans_mat = trans_mat,
  meas_covar = diag(0.01, 3),
  proc_covar = diag(0.001, 3),
  post_covar = diag(0.001, 3)
)
ggplot() +
  geom_line(data = check_forward[[1]][20:100],
            aes(x = timestamp, y = gbpusd),
            color = 'red') +
  geom_line(data = training[20:100],
            aes(x = timestamp, y = gbpusd),
            color = 'blue')
```

```{r}
check_smoother <- kalman_smoother(
  data = training[1:1000,],
  prior_state = c(270, -220, 250),
  meas_mat = trans_mat,
  prior_meas_df = 100,
  prior_meas_scale = diag(1^2, nrow = ncol(trans_mat)),
  prior_proc_df = 100,
  prior_proc_scale = diag(0.1^2, nrow = nrow(trans_mat)),
  post_covar = diag(0.01, nrow = nrow(trans_mat))
)
```

We inspect the Bayes estimates of the covariance matrices:

```{r}
check_smoother$post_meas_scale / (check_smoother$post_meas_df - nrow(trans_mat))
check_smoother$post_proc_scale / (check_smoother$post_proc_df - ncol(trans_mat))
```

And can visually inspect how the smoother performs on individual currencies:

```{r}
ggplot() +
  geom_line(data = check_smoother$predicted_measurements[500:600,],
            aes(x = timestamp, y = usdjpy), 
            color = 'red') +
  geom_line(data = check_smoother$input_data[500:600],
            aes(x = timestamp, y = usdjpy),
            color = 'blue')
```

# PSIS

PSIS is like our alarm that rings whenever the predictions become really bad and we need to update our parameters through smoothing. In particular, we fit the generalized Pareto distribution to the 20% highest importance weights, and if k > 0.7, we refit because we think the variance of the importance weights is infinity.

To do this, we sample 1000 covariance matrices at the start of our forward pass through the filter and hold those fixed (covariance matrices are very expensive to sample)

```{r}
library(MCMCpack)
library(mvtnorm)
library(loo)

# This function won't work if the priors are not reasonable
# Please run the Kalman smoother on some historical data as a starting point

kalman_filter_minibatch <- function(
  data, meas_mat, initial_state, innovation_covar, post_covar,
  prior_proc_df, prior_proc_scale,
  prior_meas_df, prior_meas_scale
){
  
  proc_cov_mats <- lapply(1:200,
                          function(x) riwish(v = prior_proc_df, S = prior_proc_scale))
  meas_cov_mats <- lapply(1:200,
                          function(x) riwish(v = prior_meas_df, S = prior_meas_scale))
  psis_k <- 0
  iter_count <- 1
  data_count <- 0
  parallel_states <- lapply(1:200, function(x) initial_state)
  innovation_covar_mats <- lapply(1:200, function(x) innovation_covar)
  post_cov_mats <- lapply(1:200, function(x) post_covar)
  importance_weights <- list()
  
  while((psis_k < 0.7) & (iter_count < nrow(data))){
    
    temp_states <- mapply(
      function(initial_state, meas_covar, proc_covar, post_covar){
        temp_file <- forward_pass(
          data = data[iter_count],
          initial_state = initial_state, 
          trans_mat = meas_mat,
          meas_covar = meas_covar,
          proc_covar = proc_covar,
          post_covar = post_covar
        )
        return(list(
          latent_states = as.numeric(
            temp_file[[2]][nrow(temp_file[[2]]),1:nrow(meas_mat)]
            ),
          innovation_covar = temp_file[[4]],
          post_covar = temp_file[[3]]
          ))
      },
      parallel_states,
      meas_cov_mats,
      proc_cov_mats,
      post_cov_mats,
      SIMPLIFY = FALSE
    )
    parallel_states <- lapply(temp_states, "[[", 1)
    innovation_covar_mats <- lapply(temp_states, "[[", 2)
    post_covar_mats <- lapply(temp_states, "[[", 3)
    
    importance_weights[[iter_count]] <- as.numeric(unlist(mapply(
      function(innovation_covar, state) dmvnorm(
        as.numeric(data[iter_count+1, 2:4]), # need to fix this
        mean = as.numeric(trans_mat %*% state),
        sigma = innovation_covar,
        log = TRUE),
      innovation_covar_mats, parallel_states
    )))
    psis_weights <- do.call('rbind', importance_weights)
#    psis_weights <- psis_weights / sd(psis_weights[1,])
#    psis_weights <- psis_weights - sort(psis_weights[1,])[161] - 18
#    psis_weights <- sort(exp(colSums(log(psis_weights))))[161:200]
    psis_weights <- sort(colSums(psis_weights))[161:200]
#    psis_weights <- (psis_weights)/10000000
    psis_weights <- exp(psis_weights)
    # we need to implement manually later
    psis_k <- gpdfit(psis_weights - min(psis_weights))$k
    iter_count <- iter_count + 1
  }
  iter_count <- iter_count - 1
  
  forward <- forward_pass(
    data = data[1:iter_count,],
    initial_state = initial_state,
    trans_mat = meas_mat, 
    meas_covar = prior_meas_scale / (prior_meas_df - nrow(meas_mat)),
    proc_covar = prior_proc_scale / (prior_proc_df - ncol(meas_mat)),
    post_covar = post_covar
  )
  return(forward)
}
```

```{r}
check_kf_minibatch <- kalman_filter_minibatch(
  data = training[1001:2000,], 
  meas_mat = trans_mat, 
  initial_state = as.numeric(check_smoother$states[nrow(check_smoother$states), 1:3]), 
  innovation_covar = check_smoother$innovation_covar, 
  post_covar = check_smoother$post_covar,
  prior_proc_df = check_smoother$post_proc_df, 
  prior_proc_scale = check_smoother$post_proc_scale,
  prior_meas_df = check_smoother$post_meas_df, 
  prior_meas_scale = check_smoother$post_meas_scale
)
ggplot(check_kf_minibatch[[2]]) +
  geom_line(aes(x = timestamp, y = JPY))

ggplot() +
  geom_line(data = check_kf_minibatch[[1]],
            aes(x = timestamp, y = usdjpy),
            color = 'red') +
  geom_line(data = training[1001:(1001+nrow(check_kf_minibatch[[1]]))],
            aes(x = timestamp, y = usdjpy),
            color = 'blue')
```

# Putting it all together

Now that we have built a function for automatically detecting when we need to update the covariance matrix, we just need to iterate through the entire dataset until the filter has passed through the entire time series.

```{r}
adaptive_kalman_filter <- function(
  data, meas_mat, burn_in = 0,
  prior_state, prior_covar, innovation_covar,
  prior_proc_df, prior_proc_scale,
  prior_meas_df, prior_meas_scale
){
  output_states <- list()
  output_meas <- list()
  output_meas_covar <- list()
  output_proc_covar <- list()
  breakpoints <- list()
  minibatch_count <- 0
  itercount <- 0
  
  if(burn_in > 0){
    smoother <- kalman_smoother(
      data = data[1:burn_in,],
      prior_state = prior_state,
      meas_mat = meas_mat,
      post_covar = prior_covar,
      prior_meas_df = prior_meas_df,
      prior_meas_scale = prior_meas_scale,
      prior_proc_df = prior_proc_df,
      prior_proc_scale = prior_proc_scale
    )
    
    itercount <- itercount + burn_in
    minibatch_count <- minibatch_count + 1
    
    breakpoints[[minibatch_count]] <- itercount
    
    output_states[[minibatch_count]] <- forward[[2]]
    output_meas[[minibatch_count]] <- fordward[[1]]
    output_meas_covar[[minibatch_count]] <- smoother[[5]] / 
      (smoother[[4]] - nrow(meas_mat))
    output_proc_covar[[minibatch_count]] <- smoother[[7]] /
      (smoother[[6]] - ncol(meas_mat))
    
    # update all our priors using the smoother
    
    batch_size <- nrow(smoother[[1]])
    batch_total_time <- sum(smoother[[1]]$time_gap)
    
    prior_state <- smoother[[2]]
    prior_state <- as.numeric(prior_state[nrow(prior_state), 1:ncol(meas_mat)])
    
    prior_meas_df <- smoother[[4]]
    prior_meas_scale <- smoother[[5]]
    
    prior_proc_df <- smoother[[6]]
    prior_proc_scale <- smoother[[7]]
    
    prior_covar <- smoother[[8]]
    innovation_covar <- smoother[[9]]
  }

  print(itercount)
  
  while(itercount < nrow(data)){
    forward <- kalman_filter_minibatch(
      data = data[(itercount+1):nrow(data),], 
      meas_mat = meas_mat, 
      initial_state = prior_state, 
      innovation_covar = innovation_covar, 
      post_covar = prior_covar,
      prior_proc_df = prior_proc_df, 
      prior_proc_scale = prior_proc_scale,
      prior_meas_df = prior_meas_df, 
      prior_meas_scale = prior_meas_scale
    )
    batch_size <- nrow(forward[[1]])
    print(paste0('batch size = ', batch_size))
    if((batch_size + itercount) >= nrow(data)){
      break
    }
    
    smoother <- kalman_smoother(
      data = data[(itercount+1):pmin(itercount+batch_size, nrow(data)),],
      prior_state = prior_state,
      meas_mat = meas_mat,
      post_covar = prior_covar,
      prior_meas_df = prior_meas_df,
      prior_meas_scale = prior_meas_scale,
      prior_proc_df = prior_proc_df,
      prior_proc_scale = prior_proc_scale
    )
    
    minibatch_count <- minibatch_count + 1
    itercount <- itercount + batch_size
    
    breakpoints[[minibatch_count]] <- itercount
    
    output_states[[minibatch_count]] <- forward[[2]]
    output_meas[[minibatch_count]] <- forward[[1]]
    output_meas_covar[[minibatch_count]] <- smoother[[5]] / 
      (smoother[[4]] - nrow(meas_mat))
    output_proc_covar[[minibatch_count]] <- smoother[[7]] /
      (smoother[[6]] - ncol(meas_mat))
    
    # update all our priors using the smoother
    
    batch_size <- nrow(smoother[[1]])
    batch_total_time <- sum(smoother[[1]]$time_gap)
    
    prior_state <- smoother[[2]]
    prior_state <- as.numeric(prior_state[nrow(prior_state), 1:ncol(meas_mat)])
    
    prior_meas_df <- smoother[[4]]
    prior_meas_scale <- smoother[[5]]
    
    prior_proc_df <- smoother[[6]]
    prior_proc_scale <- smoother[[7]]
    
    prior_covar <- smoother[[8]]
    innovation_covar <- smoother[[9]]
    
    print(itercount)
  }
  
  output_states_combined <- rbindlist(output_states)
  output_states_combined$timestamp <- shift(
    data$timestamp, n = 1, 
    type = 'lead')[1:nrow(output_states_combined)]
  
  output_meas_combined <- rbindlist(output_meas)
  output_meas_combined$timestamp <- shift(
    data$timestamp, n = 1, 
    type = 'lead')[1:nrow(output_meas_combined)]
  
  return(list(
    input_data = data,
    states = output_states_combined,
    predicted_meas = output_meas_combined,
    post_meas_df = prior_meas_df,
    post_meas_scale = prior_meas_scale,
    post_proc_df = prior_proc_df,
    post_proc_scale = prior_proc_scale,
    post_covar = prior_covar,
    innovation_covar = innovation_covar,
    breakpoints = unlist(breakpoints),
    output_meas_covar = output_meas_covar,
    output_proc_covar = output_proc_covar
  ))
}
```

```{r}
dat <- fread('./dataset/toyset/processed_toyset.csv')
dat$timestamp <- as.POSIXct(dat$timestamp)
dat$gbpjpy <- dat$gbpjpy * 100
dat$gbpusd <- dat$gbpusd * 100
dat$usdjpy <- dat$usdjpy * 100

check_smoother_init <- kalman_smoother(
  data = dat[19500:19999],
  meas_mat = trans_mat, 
  prior_state = rep(0, 3), 
  post_covar = diag(10, 3), 
  prior_proc_df = 10, 
  prior_proc_scale = diag(10 * 0.1, 3),
  prior_meas_df = 10, 
  prior_meas_scale = diag(10 * 0.1, 3)
)
```

```{r}
ggplot() +
  geom_line(data = check_smoother_init[[1]],
            aes(x = timestamp, y = gbpusd),
            color = 'blue') +
  geom_line(data = check_smoother_init[[3]],
            aes(x = timestamp, y = gbpusd),
            color = 'red')
```

```{r}
check_akf <- adaptive_kalman_filter(
  data = dat[20000:20200], 
  meas_mat = trans_mat, 
  burn_in = 0,
  prior_state = as.numeric(check_smoother_init$states[
    nrow(check_smoother_init$states), 1:ncol(trans_mat)]), 
  prior_covar = check_smoother_init$post_covar, 
  innovation_covar = check_smoother_init$innovation_covar,
  prior_proc_df = check_smoother_init$post_proc_df, 
  prior_proc_scale = check_smoother_init$post_proc_scale,
  prior_meas_df = check_smoother_init$post_meas_df, 
  prior_meas_scale = check_smoother_init$post_meas_scale
)
```

```{r}
ggplot() +
  geom_line(data = check_akf$input_data,
            aes(x = timestamp, y = gbpjpy),
            color = 'blue') +
  geom_line(data = check_akf$predicted_meas,
            aes(x = timestamp, y = gbpjpy),
            color = 'red') +
  geom_vline(aes(xintercept = check_akf$input_data$timestamp[check_akf$breakpoints]))

ggplot() +
  geom_line(data = check_akf$input_data,
            aes(x = timestamp, y = gbpusd),
            color = 'blue') +
  geom_line(data = check_akf$predicted_meas,
            aes(x = timestamp, y = gbpusd),
            color = 'red') +
  geom_vline(aes(xintercept = check_akf$input_data$timestamp[check_akf$breakpoints]))

ggplot() +
  geom_line(data = check_akf$input_data,
            aes(x = timestamp, y = usdjpy),
            color = 'blue') +
  geom_line(data = check_akf$predicted_meas,
            aes(x = timestamp, y = usdjpy),
            color = 'red') +
  geom_vline(aes(xintercept = check_akf$input_data$timestamp[check_akf$breakpoints]))
```


```{r}
ggplot(check_akf$states) +
  geom_line(aes(x = timestamp, y = GBP))

ggplot(check_akf$states) +
  geom_line(aes(x = timestamp, y = USD))

ggplot(check_akf$states) +
  geom_line(aes(x = timestamp, y = JPY))
```

```{r}
check_akf$output_meas_covar
check_akf$output_proc_covar
```