---
title: "KF prototype"
output: pdf_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=4) 
```

# Cleaning the data

```{r, warning = FALSE, message = FALSE}
library(data.table)
library(lubridate)
library(zoo)
library(ggplot2)

var_names <- c('timestamp',  'opening', 'high', 'low', 'close', 'volume')
dir <- './dataset/toyset/'

custom_merge <- function(x, y){
  return(merge(x, y, by = 'timestamp', all = TRUE))
}

create_timestamps <- function(start_date, end_date, opening_time, closing_time){
  date_seq <- seq.Date(start_date, end_date, by = 'day')
  temp <- list()
  for(i in 1:length(date_seq)){
    if(!is.na(opening_time[wday(date_seq[i])])){
      temp[[i]] <- data.table(
        timestamp = seq.POSIXt(
          ymd_hms(paste(date_seq[i], opening_time[wday(date_seq[i])])), 
          ymd_hms(paste(date_seq[i], closing_time[wday(date_seq[i])])),
          by = 'min'
        )
      )
    }
  }
  return(rbindlist(temp))
}

get_data <- function(dir){
  filenames <- list.files(dir)
  filenames <- filenames[!grepl("processed", filenames)]
  dat <- list()
  for(i in 1:length(filenames)){
    temp_dat <- fread(paste0(dir, filenames[i]))
    setnames(temp_dat, var_names)
    temp_dat2 <- temp_dat[,.(
      timestamp = as.POSIXct(timestamp, format = "%Y%m%d %H%M%OS", tz = 'EST'),
      price = log(close)
      )]
    setnames(temp_dat2, c('timestamp', substr(filenames[i], 1, 6)))
    dat[[i]] <- temp_dat2
  }
  final_dat <- Reduce(custom_merge, dat)
  
  daily_times <- final_dat[
    ,.(opening_time = substr(timestamp, 12, 19), 
       closing_time = substr(timestamp, 12, 19),
       timestamp = as.Date(timestamp, tz = 'EST'))]
  daily_times <- daily_times[
    ,.(opening_time = head(sort(opening_time), 1),
       closing_time = tail(sort(closing_time), 1)),
    by = .(dow = wday(timestamp))
  ]
  daily_times <- daily_times[order(dow)]
  
  timestamp_list <- create_timestamps(
    start_date   = min(as.Date(final_dat$timestamp, tz = 'EST')),
    end_date     = max(as.Date(final_dat$timestamp, tz = 'EST')),
    opening_time = daily_times$opening_time[match(1:7, daily_times$dow)],
    closing_time = daily_times$closing_time[match(1:7, daily_times$dow)]
  )
  
  final_dat <- merge(timestamp_list, final_dat, by = 'timestamp', all = TRUE)
  final_dat <- final_dat[,lapply(.SD, na.locf, na.rm = FALSE, fromLast = TRUE)]
  return(final_dat)
}

dat <- get_data(dir)
dat[,time_gap := (as.numeric(timestamp) - as.numeric(shift(timestamp)))/60]
dat$time_gap[1] <- 1
head(dat)
```

```{r}
write.csv(dat, './dataset/toyset/processed_toyset.csv', row.names = FALSE)
```

# Kalman Filter with known covariances

Here we assume independence in movements, just to get a simple prototype to work. proc_covar and meas_covar are the main settings. When proc_covar is relatively small when compared to meas_covar, then the filter will be smooth.

```{r}
proc_covar <- matrix(0, nrow = 3, ncol = 3)
diag(proc_covar) <- 0.0001^2

post_covar <- proc_covar

meas_covar <- matrix(0, nrow = 3, ncol = 3)
diag(meas_covar) <- 0.0003^2

pred_covar <- meas_covar

ident <- matrix(0, nrow = 3, ncol = 3)
diag(ident) <- 1

# assume the order of measurements is GBPJPY, GBPUSD, USDJPY
# and order of latent variables is GBP, JPY, USD

state_old <- matrix(rep(1, 3), ncol = 1)
trans_mat <- matrix(c(
  1, -1, 0,
  1, 0, -1,
  0, -1, 1
), nrow = 3, ncol = 3, byrow = TRUE)

obs_mat <- as.matrix(dat[,!colnames(dat) %in% c('timestamp', 'time_gap'), with = FALSE])
time_gap <- dat$time_gap

latent_states <- list()
predicted_obs <- list()

for(i in 1:nrow(dat)){
  post_covar <- post_covar + time_gap[i] * proc_covar
  innovation <- obs_mat[i,] - trans_mat %*% state_old
  innovation_covar <- trans_mat %*% post_covar %*% t(trans_mat) + meas_covar
  kalman_gain <- post_covar %*% t(trans_mat) %*% solve(innovation_covar)
  state_new <- state_old + kalman_gain %*% innovation
  post_covar <- (ident - kalman_gain %*% trans_mat) %*% post_covar
  predicted_obs[[i]] <- t(trans_mat %*% state_new)
  latent_states[[i]] <- t(state_new)
  state_old <- state_new
}

predictions <- data.table(do.call(rbind, predicted_obs))
setnames(predictions, colnames(obs_mat))
predictions$timestamp <- shift(dat$timestamp, n = 1, type = 'lead')

latent_estimates <- data.table(do.call(rbind, latent_states))
setnames(latent_estimates, c('GBP', 'JPY', 'USD'))
latent_estimates$timestamp <- shift(dat$timestamp, n = 1, type = 'lead')
```

## How good are the predictions?

```{r}
# baseline of one-step ahead forecast
baseline <- data.table(
  gbpjpy = dat$gbpjpy - shift(dat$gbpjpy, n = 1, type = 'lead'),
  gbpusd = dat$gbpusd - shift(dat$gbpusd, n = 1, type = 'lead'),
  usdjpy = dat$usdjpy - shift(dat$usdjpy, n = 1, type = 'lead')
)
baseline <- baseline[200:nrow(baseline)]
baseline[,lapply(.SD, function(x) sqrt(mean(x^2, na.rm = TRUE)))]

predicted <- data.table(
  gbpjpy = predictions$gbpjpy - shift(dat$gbpjpy, n = 1, type = 'lead'),
  gbpusd = predictions$gbpusd - shift(dat$gbpusd, n = 1, type = 'lead'),
  usdjpy = predictions$usdjpy - shift(dat$usdjpy, n = 1, type = 'lead')
)
predicted <- predicted[200:nrow(predicted)]
predicted[,lapply(.SD, function(x) sqrt(mean(x^2, na.rm = TRUE)))]


```

It's worse than the baseline. Let's see what's going on:

```{r}
viz_dat <- merge(
  melt(dat[,.(timestamp, gbpjpy, gbpusd, usdjpy)], 
       id.vars = 'timestamp', 
       variable.name = 'currency',
       value.name = 'actual'), 
  melt(predictions[,.(timestamp, gbpjpy, gbpusd, usdjpy)],
       id.vars = 'timestamp',
       variable.name = 'currency',
       value.name = 'predicted'),
  by = c('timestamp', 'currency')
)
ggplot(viz_dat[currency == 'gbpusd' &
                 timestamp >= '2019-05-14 12:00:00' & 
                 timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = actual), color = 'blue') +
  geom_line(aes(x = timestamp, y = predicted), color = 'red') +
  ggtitle('GBPUSD')


ggplot(viz_dat[currency == 'gbpjpy' &
                 timestamp >= '2019-05-14 12:00:00' & 
                 timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = actual), color = 'blue') +
  geom_line(aes(x = timestamp, y = predicted), color = 'red') +
  ggtitle('GBPJPY')


ggplot(viz_dat[currency == 'usdjpy' &
                 timestamp >= '2019-05-14 12:00:00' & 
                 timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = actual), color = 'blue') +
  geom_line(aes(x = timestamp, y = predicted), color = 'red') +
  ggtitle('USDJPY')
```

As we expected, currency exchange rates are extremely volatile, and our naive approach fails. Then again, the covariance matrices are not estimated in this case, and there may be a setting that works better. But on the bright side, the graph of currency intrinsic values is really neat:

```{r}
latent_viz <- melt(
  latent_estimates,
  id.vars = 'timestamp',
  variable.name = 'currency',
  value.name = 'log_intrinsic_value'
)
ggplot(latent_viz[currency == 'JPY' &
                  timestamp >= '2019-05-14 12:00:00' & 
                    timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = log_intrinsic_value)) +
  ggtitle('Intrinsic value of JPY')

ggplot(latent_viz[currency == 'USD' &
                  timestamp >= '2019-05-14 12:00:00' & 
                    timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = log_intrinsic_value)) +
  ggtitle('Intrinsic value of USD')

ggplot(latent_viz[currency == 'GBP' &
                  timestamp >= '2019-05-14 12:00:00' & 
                    timestamp <= '2019-05-14 14:00:00']) +
  geom_line(aes(x = timestamp, y = log_intrinsic_value)) +
  ggtitle('Intrinsic value of GBP')
```


# Parameter Estimation



```{r}
training <- dat[10001:11000,]

# E step

forward_pass <- function(
  data, initial_state, trans_mat, meas_covar, proc_covar
){
  obs_mat <- as.matrix(data[,!colnames(dat) %in% c('timestamp', 'time_gap'), 
                            with = FALSE])
  time_gap <- data$time_gap
  
  latent_states <- list()
  predicted_obs <- list()
  
  for(i in 1:nrow(data)){
    post_covar <- post_covar + time_gap[i] * proc_covar
    innovation <- obs_mat[i,] - trans_mat %*% state_old
    innovation_covar <- trans_mat %*% post_covar %*% t(trans_mat) + meas_covar
    kalman_gain <- post_covar %*% t(trans_mat) %*% solve(innovation_covar)
    state_new <- state_old + kalman_gain %*% innovation
    post_covar <- (ident - kalman_gain %*% trans_mat) %*% post_covar
    predicted_obs[[i]] <- t(trans_mat %*% state_new)
    latent_states[[i]] <- t(state_new)
    state_old <- state_new
  }
    
  predictions <- data.table(do.call(rbind, predicted_obs))
  setnames(predictions, colnames(obs_mat))
  predictions$timestamp <- shift(data$timestamp, n = 1, type = 'lead')
  predictions$time_gap <- time_gap
  
  latent_estimates <- data.table(do.call(rbind, latent_states))
  setnames(latent_estimates, c('GBP', 'JPY', 'USD'))
  latent_estimates$timestamp <- shift(data$timestamp, n = 1, type = 'lead')
  latent_estimates$time_gap <- time_gap
  
  return(list(predictions, latent_estimates))
}

check <- forward_pass(
  data = training[1:100,],
  initial_state = rep(1, 3),
  trans_mat = trans_mat,
  meas_covar = meas_covar,
  proc_covar = proc_covar
)

# M step

sample_proc_covar <- function(data){
  data <- data[complete.cases(data)]
  time_gap <- data$time_gap
  data <- data[,timestamp := NULL]
  data <- data[,time_gap := NULL]
  sample_means <- as.numeric(
    data[,lapply(.SD, function(x) sum(x * time_gap) / sum(time_gap))]
    )
  data <- t((t(as.matrix(data / time_gap)) - sample_means))
  data <- t(data) %*% data
  return(data)
}

sample_meas_covar <- function(pred, obs){
  unique_timestamp <- sort(unique(pred$timestamp, obs$timestamp))
  obs_mat <- obs[timestamp %in% unique_timestamp]
  pred_mat <- pred[timestamp %in% unique_timestamp]
  
  obs_mat <- as.matrix(obs_mat[order(timestamp), !c('timestamp', 'time_gap')])
  pred_mat <- as.matrix(pred_mat[order(timestamp),][, colnames(obs_mat), with = FALSE])
  
  error_mat <- obs_mat - pred_mat
  cov_mat <- cov(error_mat)
  
  return(cov_mat)
}

check3 <- sample_proc_covar(check[[2]])
check4 <- sample_meas_covar(pred = check[[1]], obs = training[1:100])

# EM

kalman_smoother <- function(
  data, prior_state, meas_mat, prior_df, prior_meas_scale, prior_proc_scale
){
  forward <- forward_pass(
    data = data, 
    initial_state = prior_state,
    trans_mat = trans_mat,
    meas_covar = prior_meas_scale / (prior_df),
    proc_covar = prior_proc_scale / (prior_df)
  )
  proc_covar_new <- sample_proc_covar(forward[[2]])
  meas_covar_new <- sample_meas_covar(pred = forward[[1]], obs = data)
  
  proc_covar_old <- proc_covar_new * 2
  meas_covar_old <- meas_covar_new * 2
  
  while(sqrt(sum((proc_covar_old - proc_covar_new)^2) +
        sum((meas_covar_old - meas_covar_new)^2)) > 10^(-16)){
    print(
      sum((proc_covar_old - proc_covar_new)^2) +
        sum((meas_covar_old - meas_covar_new)^2)
    )
    
    meas_covar_old <- meas_covar_new
    proc_covar_old <- proc_covar_new
    
    forward <- forward_pass(
      data = data,
      initial_state = prior_state,
      trans_mat = trans_mat,
      meas_covar = meas_covar_new,
      proc_covar = proc_covar_new
    )
    
    proc_covar_new <- sample_proc_covar(forward[[2]])
    meas_covar_new <- sample_meas_covar(pred = forward[[1]], obs = data)
  }
  
  new_df <- prior_df + nrow(data) - nrow(trans_mat) - 1
  
  
}
```
